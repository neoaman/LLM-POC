{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b03781",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Increase compatibility with Databricks\n",
    "from IPython.display import display as idisplay, HTML\n",
    "displayHTML = lambda x: idisplay(HTML(x))\n",
    "def display(*args, **kargs): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff586a43",
   "metadata": {},
   "source": [
    "-sandbox\n",
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c8831a",
   "metadata": {},
   "source": [
    "# LLMs and Society\n",
    "\n",
    "### ![Dolly](https://files.training.databricks.com/images/llm/dolly_small.png) Learning Objectives\n",
    "1. Learn representation bias in training data \n",
    "1. Use Hugging Face to calculate toxicity score\n",
    "1. Use SHAP to generate explanation on model output\n",
    "1. Learn the latest state of research in model explanation: contrastive explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44db438",
   "metadata": {},
   "source": [
    "## Classroom Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2449b7e7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pip install disaggregators==0.1.2 https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7382000",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a355def7",
   "metadata": {},
   "source": [
    "## Examining representation bias in Wikipedia biographies\n",
    "\n",
    "[Disaggregators](https://github.com/huggingface/disaggregators) is a library developed by Hugging Face. As the name implies, it \"dis-aggregates\" data so that we can explore the data in more granular detail and evaluate data bias.\n",
    "\n",
    "There are multiple disaggregation modules available: \n",
    "- age\n",
    "- gender\n",
    "- religion\n",
    "- continent\n",
    "- pronoun\n",
    "\n",
    "We will be loading Wikipedia bios as our datasets to analyze later. We will be using the `pronoun` module since it takes the least amount of time to dis-aggregate. You are welcome to try out other modules in your own time. \n",
    "\n",
    "**DISCLAIMER**: \n",
    "- Warning: Some content may be triggering.\n",
    "- The models developed or used in this course are for demonstration and learning purposes only. Models may occasionally output offensive, inaccurate, biased information, or harmful instructions.\n",
    "\n",
    "\n",
    "\n",
    "**IMPORTANT**:\n",
    "- For `gender` disaggregator to work, you need to download spacy's `en_core_web_lg` model. \n",
    "  - That's the model Hugging Face is using behind the scene! \n",
    "  - Hence, you can see the `.whl` file install in the `%pip install` command above. \n",
    "  - The model is directly download from [spaCy's GitHub](https://github.com/explosion/spacy-models/releases?q=en_core_web_lg)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fcf2e4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from disaggregators import Disaggregator\n",
    "\n",
    "disaggregator = Disaggregator(\"pronoun\", column=\"target_text\")\n",
    "# disaggregator = Disaggregator(\"gender\", column=\"target_text\")\n",
    "# disaggregator = Disaggregator(\"continent\", column=\"target_text\")\n",
    "# disaggregator = Disaggregator(\"religion\", column=\"target_text\")\n",
    "# disaggregator = Disaggregator(\"age\", column=\"target_text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7f8257",
   "metadata": {},
   "source": [
    "We will use [Wikipedia biographies dataset](https://huggingface.co/datasets/wiki_bio), `wiki_bio`, readily available in the Hugging Face Datasets. From the dataset summary, the data contains the first paragraph of the biography and the tabular infobox. \n",
    "\n",
    "As you see, disaggreator works with Hugging Face datasets or any datasets where `.map` can be invoked. The `disaggregators` library attempts to group the wiki bio into `she_her`, `he_him`, and `they_them`.\n",
    "\n",
    "Note: the cell below might take a couple minutes for the data to finish loading and for the disaggregator to categorize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc68d2f0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wiki_data = load_dataset(\n",
    "    \"wiki_bio\", split=\"test\", cache_dir=DA.paths.datasets\n",
    ")  # Note: We specify cache_dir to use pre-cached data.\n",
    "ds = wiki_data.map(disaggregator)\n",
    "pdf = ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c213eb37",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's take a look at the dataframe\n",
    "pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1b9a6f",
   "metadata": {},
   "source": [
    "However, it doesn't do a very a good job at determining `they_them` as it seems to classify mentions of physical objects as `they_them` as well. For example, the 19th row has both pronoun categories, `they_them` and `he_him`, to be true. But looking at the data itself, we saw that the bio only used the the pronoun `he_him`: \n",
    "\n",
    " >william ` bill ' rigby -lrb- 9 june 1921 - 1 june 2010 -rrb- was a former english footballer who played as a goalkeeper .\\nhe was born in chester .\\na product of the youth system at his hometown club of chester , rigby made his only peacetime first-team appearance for the club in their first post-war match in the football league in a 4 -- 4 draw at york city on 31 august 1946 .\\nafter this he was not selected again , with goalkeeping duties being passed on to george scales and jim maclaren .\\nearlier he had made appearances for the first-team during the war years , mainly during 1940 -- 41 and 1941 -- 42 while understudy to bill shortt\n",
    "\n",
    "For this reason, the following analysis will ignore the column `pronoun.they_them`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab67201",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print(pdf.iloc[[19], :].to_json(indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda325fd",
   "metadata": {},
   "source": [
    "Let's do a simple aggregation to check the ratio of Wikipedian bios in terms of `he_him` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aa6790",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "she_array = np.where(pdf[\"pronoun.she_her\"] == True)\n",
    "print(f\"she_her: {len(she_array[0])} rows\")\n",
    "he_array = np.where(pdf[\"pronoun.he_him\"] == True)\n",
    "print(f\"he_him: {len(he_array[0])} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4846fe7f",
   "metadata": {},
   "source": [
    "The `he_him` pronoun represents `44004/ (9545+44004)`, which is approximately 82% of the data! It is not hard to imagine that models trained on predominantly male data would exhibit bias towards males.\n",
    "\n",
    "Let's confirm that existing pre-trained models, like BERT, does exhibit bias. BERT is trained on both Wikipedia and [books that are adapted into movies](https://huggingface.co/datasets/bookcorpus). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2cb6ea",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"bert-base-uncased\",\n",
    "    model_kwargs={\"cache_dir\": DA.paths.datasets},\n",
    ")  # Note: We specify cache_dir to use pre-cached models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466d49b7",
   "metadata": {},
   "source": [
    "To probe what BERT outputs, we will intentionally insert [MASK] token and ask BERT to generate words to replace that [MASK] token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334eafd8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = unmasker(\"This woman works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])\n",
    "\n",
    "result = unmasker(\"This man works as a [MASK].\")\n",
    "print([r[\"token_str\"] for r in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3844129e",
   "metadata": {},
   "source": [
    "## Inspect toxicity\n",
    "\n",
    "Now that we have inspected data and model bias above, let's evaluate the toxicity of language model outputs. To do this, we leverage another [Hugging Face library called `evaluate`](https://huggingface.co/docs/evaluate/index).\n",
    "\n",
    "The `evaluate` library can measure language models from different angles:\n",
    "<br>\n",
    "- Toxicity: how problematic the completion is, such as hate speech\n",
    "  - It uses [Facebook's `roberta-hate-speech-dynabench-r4-target` model](https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target) behind the scene to compute toxicity.\n",
    "- [HONEST](https://huggingface.co/spaces/evaluate-measurement/honest): how hurtful the completion is \n",
    "  - The model was [published in 2021](https://aclanthology.org/2021.naacl-main.191.pdf)\n",
    "  - It works very similarly as our `unmasker` example in the cell directly above. It also replaces certain words with [MASK] tokens and evaluates the hurtfulness based on what the language models output.\n",
    "- Regard: whether the completion regards a certain group higher than the others \n",
    "  - You will play with this in the lab! So we will save this for later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460823e6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "toxicity = evaluate.load(\"toxicity\", module_type=\"measurement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eda6039",
   "metadata": {},
   "source": [
    "Any toxicity value over 0.5 is arbitrarily defined as \"toxic\". Here, we refrain from typing literal curse words to increase the toxicity values. However, you can see that the third phrase is noticeably more toxic than the other two! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbbf22c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "candidates = [\n",
    "    \"their kid loves reading books\",\n",
    "    \"she curses and makes fun of people\",\n",
    "    \"he is a wimp and pathetic loser\",\n",
    "]\n",
    "toxicity.compute(predictions=candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3518028",
   "metadata": {},
   "source": [
    "## Model Interpretability with SHAP \n",
    "\n",
    "Another interesting topic within language model evaluation is whether we can interpret LM outputs. **SH**apley **A**dditive ex**P**lanations (**SHAP**) is a popular approach to explain the output of a machine learning model. It is agnostic to the type of machine learning model you pass in; this means that we can try using SHAP to explain our language model outputs! \n",
    "\n",
    "See the <a href=\"http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions\" target=\"_blank\">SHAP NeurIPS</a> paper for details, and Christoph Molnar's book chapter on <a href=\"https://christophm.github.io/interpretable-ml-book/shapley.html\" target=\"_blank\">Shapley Values</a>. \n",
    "\n",
    "Take the diagram below as an example. SHAP's goal is to explain the $10,000 difference in the apartment price. We see that if cats are not allowed in the same apartment building, the price of the apartment is lower than if it were to allow cats. \n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/shap_permutation.png\" width=500>\n",
    "\n",
    "Image is sourced from Molnar's book. Read SHAP [documentation here](https://shap.readthedocs.io/en/latest/text_examples.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617995cb",
   "metadata": {},
   "source": [
    "In this section, we are going to first load a text generation model from Hugging Face, provide an input sentence, and ask the model to complete the rest of the sentence. Then, we will ask SHAP to generate explanation behind the sentence completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184f1cda",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import shap\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"gpt2\", use_fast=True, cache_dir=DA.paths.datasets\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\", cache_dir=DA.paths.datasets)\n",
    "\n",
    "# Set model decoder to true\n",
    "# GPT is a decoder-only model\n",
    "model.config.is_decoder = True\n",
    "# We set configurations for the output text generation\n",
    "model.config.task_specific_params[\"text-generation\"] = {\n",
    "    \"do_sample\": True,\n",
    "    \"max_length\": 50,\n",
    "    \"temperature\": 0,  # to turn off randomness\n",
    "    \"top_k\": 50,\n",
    "    \"no_repeat_ngram_size\": 2,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b720e813",
   "metadata": {},
   "source": [
    "Feel free to modify the input sentence below to play around later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0485f6f7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_sentence = [\"Sunny days are the best days to go to the beach. So\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db314702",
   "metadata": {},
   "source": [
    "The `shap.Explainer` is how we can interface with SHAP. We need to pass in our `tokenizer` because that's the tokenizer we use to vectorize the text. When SHAP masks certain tokens to generate explanation, the tokenizer helps us to retain the same number of tokens by replacing the word with the [MASK] token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2b7d6e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model, tokenizer)\n",
    "shap_values = explainer(input_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdebb6c",
   "metadata": {},
   "source": [
    "Now we can check the contribution of each input token towards the output token. \"Red\" means positive contribution whereas \"blue\" means negative indication. The color intensity indicates the strength of the contribution. \n",
    "\n",
    "From the documentation:\n",
    "\n",
    "> The base value is what the model outputs when the entire input text is masked, while f_outputclass(inputs)\n",
    " is the output of the model for the full original input. The SHAP values explain in an additive way how the impact of unmasking each word changes the model output from the base value (where the entire input is masked) to the final prediction value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a36238",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shap.plots.text(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f7fb78",
   "metadata": {},
   "source": [
    "The plot below shows which input tokens contributes most towards the output token `looking`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66839984",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values[0, :, \"looking\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090d4b50",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_sentence2 = [\"I know many people who prefer beaches to the mountains\"]\n",
    "shap_values2 = explainer(input_sentence2)\n",
    "shap.plots.text(shap_values2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b12576",
   "metadata": {},
   "source": [
    "Let's check which token contributes the most to the output word \"not\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c98539",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values2[0, :, \"not\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5216dec",
   "metadata": {},
   "source": [
    "Common model interpretability methods for text classification are not as informative for language model predictions because the most recent input token usually is the most influential token to the subsequent predicted token. This is called recency bias and it's a difficult problem to tackle. While SHAP gave us an idea what input token may have contributed to the output token, it's not really all that useful. \n",
    "\n",
    "Let's take a look at the final example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0782ab7e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_sentence3 = [\"Can you stop the dog from\"]\n",
    "shap_values3 = explainer(input_sentence3)\n",
    "shap.plots.text(shap_values3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924df609",
   "metadata": {},
   "source": [
    "In the example above, we see that the predicted token is `barking`. But we don't know why the model doesn't output tokens like `crying`, `eating`, `biting`, etc. It would be a lot more interesting if we can know *why* the model outputs `barking` **instead of** `crying` and other viable word candidates. This `instead of` explanation is called `contrastive explanation` ([Yin and Neubig 2022](https://aclanthology.org/2022.emnlp-main.14.pdf)). \n",
    "\n",
    "\n",
    "Let the actual output token be `target_token` and the viable output token be `foil_token`. Intuitively, there are three methods to generate such contrastive explanations: \n",
    "1. Calculate how much an input token influences the probability of `target_token`, while decreasing the probability of `foil_token`\n",
    "2. Calculate how much erasing an input token increases the probability of `foil_token` and reduces that of `target_token` \n",
    "3. Calculate the dot product between the input token embedding and the output token. \n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/constrastive_exp.png\" width=300>\n",
    "\n",
    "Courtesy of the author's, Kayo Yin's, [slides](https://kayoyin.github.io/assets/slides/melb22.pdf). Below, we are going to use Yin's [Python module](https://github.com/kayoyin/interpret-lm/tree/main) to generate contrastive explanation for us! The code is currently in a research state, rather than readily packaged on PyPI or production-ready, but it is still interesting to see current (and potential future) state of research directions.\n",
    "\n",
    "We will walk through the results directly in the markdown. If you are interested in running this code, you can download `lm_saliency.py` from the [repo](https://github.com/kayoyin/interpret-lm/blob/main/lm_saliency.py) and import it into your `LLM 05 - LLMs and Society` folder. \n",
    "\n",
    "```\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    cache_dir=DA.paths.datasets)\n",
    "gpt2_model = GPT2LMHeadModel.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    cache_dir=DA.paths.datasets)\n",
    "\n",
    "input_tokens = gpt2_tokenizer(input_sentence3[0])[\"input_ids\"]\n",
    "attention_ids = gpt2_tokenizer(input_sentence3[0])[\"attention_mask\"]\n",
    "```\n",
    "\n",
    "Recall that `input_sentence3[0]` is `Can you stop the dog from`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ca745b",
   "metadata": {},
   "source": [
    "We will use the erasure method to generate explanation. \n",
    "\n",
    "```\n",
    "import lm_saliency\n",
    "from lm_saliency import *\n",
    "\n",
    "target = \"barking\" # target refers to the word we would like to generate explanation on\n",
    "foil = \"crying\" # foil refers to any other possible word \n",
    "explanation = \"erasure\"\n",
    "CORRECT_ID = gpt2_tokenizer(\" \" + target)[\"input_ids\"][0]\n",
    "FOIL_ID = gpt2_tokenizer(\" \" + foil)[\"input_ids\"][0]\n",
    "\n",
    "# Erasure\n",
    "base_explanation = erasure_scores(gpt2_model, input_tokens, attention_ids, normalize=True)\n",
    "contra_explanation = erasure_scores(gpt2_model, input_tokens, attention_ids, correct=CORRECT_ID, foil=FOIL_ID, normalize=True)\n",
    "\n",
    "visualize(np.array(base_explanation), gpt2_tokenizer, [input_tokens], print_text=True, title=f\"Why did the model predict {target}?\")\n",
    "visualize(np.array(contra_explanation), gpt2_tokenizer, [input_tokens], print_text=True, title=f\"Why did the model predict {target} instead of {foil}?\")\n",
    "```\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/llm/lm_saliency.png\" width=1000>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e472ddf7",
   "metadata": {},
   "source": [
    "The score measures how much each token influences the model to attribute a higher probability to the target token. In this example above, `stop` makes the model more likely to predict `barking` whereas `the` doesn't influence whether the model predicts `barking` or `crying`.  \n",
    "\n",
    "\n",
    "How we can use contrastive explanation to improve LLMs is still an ongoing research! It's not surprising that the research so far has shown that contrastive explanation can help us characterize how LLMs decide which output token to predict. It's an exciting space to watch for development! "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
