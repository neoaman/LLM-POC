{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f03c59",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Increase compatibility with Databricks\n",
    "from IPython.display import display as idisplay, HTML\n",
    "displayHTML = lambda x: idisplay(HTML(x))\n",
    "def display(*args, **kargs): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190c017c",
   "metadata": {},
   "source": [
    "-sandbox\n",
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ef82b6",
   "metadata": {},
   "source": [
    "# 02L - Embeddings, Vector Databases, and Search\n",
    "\n",
    "\n",
    "In this lab, we will apply the text vectorization, search, and question answering workflow that you learned in the demo. The dataset we will use this time will be on talk titles and sessions from [Data + AI Summit 2023](https://www.databricks.com/dataaisummit/). \n",
    "\n",
    "### ![Dolly](https://files.training.databricks.com/images/llm/dolly_small.png) Learning Objectives\n",
    "1. Learn how to use Chroma to store your embedding vectors and conduct similarity search\n",
    "1. Use OpenAI GPT-3.5 to generate response to your prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e2c072",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pip install chromadb==0.3.21 tiktoken==0.3.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f7a1b4",
   "metadata": {},
   "source": [
    "## Classroom Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d8c075",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bf28ee",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f3752d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dais_pdf = pd.read_parquet(f\"{DA.paths.datasets}/dais/dais23_talks.parquet\")\n",
    "display(dais_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a861b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dais_pdf[\"full_text\"] = dais_pdf.apply(\n",
    "    lambda row: f\"\"\"Title: {row[\"Title\"]}\n",
    "                Abstract:  {row[\"Abstract\"]}\"\"\".strip(),\n",
    "    axis=1,\n",
    ")\n",
    "print(dais_pdf.iloc[0][\"full_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe067d66",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = dais_pdf[\"full_text\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a6454e",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "Set up Chroma and create collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677b8f55",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "chroma_client = chromadb.Client(\n",
    "    Settings(\n",
    "        chroma_db_impl=\"duckdb+parquet\",\n",
    "        persist_directory=DA.paths.user_db,  # this is an optional argument. If you don't supply this, the data will be ephemeral\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fc5f00",
   "metadata": {},
   "source": [
    "Assign the value of `my_talks` to the `collection_name` variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f86da00",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "collection_name = \"<FILL_IN>\"\n",
    "\n",
    "# If you have created the collection before, you need to delete the collection first\n",
    "if len(chroma_client.list_collections()) > 0 and collection_name in [chroma_client.list_collections()[0].name]:\n",
    "    chroma_client.delete_collection(name=collection_name)\n",
    "else:\n",
    "    print(f\"Creating collection: '{collection_name}'\")\n",
    "    talks_collection = chroma_client.create_collection(name=collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d910d6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion2_1(collection_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26498bc3",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "[Add](https://docs.trychroma.com/reference/Collection#add) data to the collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd2efac",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "talks_collection.add(\n",
    "    documents=<FILL_IN>,\n",
    "    ids=<FILL_IN>\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c499d035",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion2_2(talks_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cafa40b",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "[Query](https://docs.trychroma.com/reference/Collection#query) for relevant documents. If you are looking for talks related to language models, your query texts could be `language models`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de1e794",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "import json\n",
    "\n",
    "results = talks_collection.query(\n",
    "    query_texts=<FILL_IN>,\n",
    "    n_results=<FILL_IN>\n",
    ")\n",
    "\n",
    "print(json.dumps(results, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad6f568",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion2_3(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3fe442",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Load a language model and create a [pipeline](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbb2e83",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Pick a model from HuggingFace that can generate text\n",
    "model_id = \"<FILL_IN>\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "lm_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"<FILL_IN>\", model=lm_model, tokenizer=tokenizer, max_new_tokens=512, device_map=\"auto\", handle_long_generation=\"hole\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61e423d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion2_4(pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e1f066",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Prompt engineering for question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83d2159",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "# Come up with a question that you need the LLM assistant to help you with\n",
    "# A sample question is \"Help me find sessions related to XYZ\" \n",
    "# Note: Your \"XYZ\" should be related to the query you passed in Question 3. \n",
    "question = \"<FILL_IN>\"\n",
    "\n",
    "# Provide all returned similar documents from the cell above below\n",
    "context = <FILL_IN>\n",
    "\n",
    "# Feel free to be creative how you construct the prompt. You can use the demo notebook as a jumpstart reference.\n",
    "# You can also provide more requirements in the text how you want the answers to look like.\n",
    "# Example requirement: \"Recommend top-5 relevant sessions for me to attend.\"\n",
    "prompt_template = <FILL_IN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7b48d2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion2_5(question, context, prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34bf0bc",
   "metadata": {},
   "source": [
    "## Question 6 \n",
    "\n",
    "Submit query for language model to generate response.\n",
    "\n",
    "Hint: If you run into the error `index out of range in self`, make sure to check out this [documentation page](https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.TextGenerationPipeline.__call__.handle_long_generation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b6e027",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "lm_response = pipe(<FILL_IN>)\n",
    "print(lm_response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93090149",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test your answer. DO NOT MODIFY THIS CELL.\n",
    "\n",
    "dbTestQuestion2_6(lm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fa4e66",
   "metadata": {},
   "source": [
    "Notice that the output isn't exactly helpful. Head on to using OpenAI to try out GPT-3.5 instead! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac109e0",
   "metadata": {},
   "source": [
    "## OPTIONAL (Non-Graded): Use OpenAI models for Q/A\n",
    "\n",
    "For this section to work, you need to generate an Open AI key. \n",
    "\n",
    "Steps:\n",
    "1. You need to [create an account](https://platform.openai.com/signup) on OpenAI. \n",
    "2. Generate an OpenAI [API key here](https://platform.openai.com/account/api-keys). \n",
    "\n",
    "Note: OpenAI does not have a free option, but it gives you $5 as credit. Once you have exhausted your $5 credit, you will need to add your payment method. You will be [charged per token usage](https://openai.com/pricing). **IMPORTANT**: It's crucial that you keep your OpenAI API key to yourself. If others have access to your OpenAI key, they will be able to charge their usage to your account! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc1e1f0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<FILL IN>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d92817",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e575861",
   "metadata": {},
   "source": [
    "If you would like to estimate how much it would cost to use OpenAI, you can use `tiktoken` library from OpenAI to get the number of tokens from your prompt.\n",
    "\n",
    "\n",
    "We will be using `gpt-3.5-turbo` since it's the most economical option at ($0.002/1k tokens), as of May 2023. GPT-4 charges $0.04/1k tokens. The following code block below is referenced from OpenAI's documentation on [\"Managing tokens\"](https://platform.openai.com/docs/guides/chat/managing-tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0733263",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "price_token = 0.002\n",
    "encoder = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "cost_to_run = len(encoder.encode(prompt_template)) / 1000 * price_token\n",
    "print(f\"It would take roughly ${round(cost_to_run, 5)} to run this prompt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6884b036",
   "metadata": {},
   "source": [
    "We won't have to create a new vector database again. We can just send our `context` from above to OpenAI. We will use their chat completion API to interact with `GPT-3.5-turbo`. You can refer to their [documentation here](https://platform.openai.com/docs/guides/chat).\n",
    "\n",
    "Something interesting is that OpenAI models use the system message to help their assistant to be more accurate. From OpenAI's [docs](https://platform.openai.com/docs/guides/chat/introduction):\n",
    "\n",
    "> Future models will be trained to pay stronger attention to system messages. The system message helps set the behavior of the assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd7b79",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "gpt35_response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": <FILL_IN>},\n",
    "    ],\n",
    "    temperature=0, # 0 makes outputs deterministic; The closer the value is to 1, the more random the outputs are for each time you re-run.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c39b8a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(gpt35_response.choices[0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fa8886",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "Markdown(gpt35_response.choices[0][\"message\"][\"content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42498f86",
   "metadata": {},
   "source": [
    "We can also check how many tokens OpenAI has used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48a1fa4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpt35_response[\"usage\"][\"total_tokens\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae6f8f4",
   "metadata": {},
   "source": [
    "The results are noticeably much better compared to when using Hugging Face's GPT-2! It didn't get stuck in the text generation, but the sessions recommended are not all relevant to pandas either. You can further do more prompt engineering to get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0347b07d",
   "metadata": {},
   "source": [
    " ## Submit your Results (edX Verified Only)\n",
    "\n",
    "To get credit for this lab, click the submit button in the top right to report the results. If you run into any issues, click `Run` -> `Clear state and run all`, and make sure all tests have passed before re-submitting. If you accidentally deleted any tests, take a look at the notebook's version history to recover them or reload the notebooks."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
