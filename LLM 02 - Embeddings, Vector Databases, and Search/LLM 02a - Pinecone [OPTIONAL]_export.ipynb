{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b708ebc1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Increase compatibility with Databricks\n",
    "from IPython.display import display as idisplay, HTML\n",
    "displayHTML = lambda x: idisplay(HTML(x))\n",
    "def display(*args, **kargs): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5344aff7",
   "metadata": {},
   "source": [
    "-sandbox\n",
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cfa07f",
   "metadata": {},
   "source": [
    "# Pinecone\n",
    "\n",
    "In this section, we are going to try out another vector database, called Pinecone. It has a free tier which you need to sign up for to gain access below.\n",
    "\n",
    "Pinecone is a cloud-based vector database that offers fast and scalable similarity search for high-dimensional data, with a focus on simplicity and ease of use. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dd55f9",
   "metadata": {},
   "source": [
    "## Library pre-requisites\n",
    "\n",
    "- pinecone-client\n",
    "  - pip install below\n",
    "- Spark connector jar file\n",
    "  - **IMPORTANT!!** Since we will be interacting with Spark by writing a Spark dataframe out to Pinecone, we need a Spark Connector.\n",
    "  - You need to attach a Spark-Pinecone connector `s3://pinecone-jars/spark-pinecone-uberjar.jar` in the cluster you are using. Refer to this [documentation](https://docs.pinecone.io/docs/databricks#setting-up-a-spark-cluster) if you need more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83e5957",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pip install pinecone-client==2.2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6b0641",
   "metadata": {},
   "source": [
    "## Classroom Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e61fd5c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1db0b84",
   "metadata": {},
   "source": [
    "### Setting up your Pinecone\n",
    "\n",
    "Step 1: Go to their [home page](https://www.pinecone.io/) and click `Sign Up Free` on the top right corner. \n",
    "<br>\n",
    "Step 2: Click on `Sign Up`. It's possible that you may not be able to sign up for a new account, depending on Pinecone's availability. \n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/pinecone_register.png\" width=300>\n",
    "\n",
    "Step 3: Once you are in the console, navigate to `API Keys` and copy the `Environment` and `Value` (this is your API key).\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/pinecone_credentials.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecf8eb1",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "import os\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"<FILL IN>\"\n",
    "os.environ[\"PINECONE_ENV\"] = \"<FILL IN>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd804a5c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "pinecone_api_key = os.environ[\"PINECONE_API_KEY\"]\n",
    "pinecone_env = os.environ[\"PINECONE_ENV\"]\n",
    "\n",
    "pinecone.init(api_key=pinecone_api_key, environment=pinecone_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd95c10e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .option(\"sep\", \";\")\n",
    "    .format(\"csv\")\n",
    "    .load(\n",
    "        f\"{DA.paths.datasets}/news/labelled_newscatcher_dataset.csv\".replace(\n",
    "            \"/dbfs\", \"dbfs:\"\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"id\", F.expr(\"uuid()\"))\n",
    ")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d240af4",
   "metadata": {},
   "source": [
    "For Pinecone, we need to generate the embeddings first and save it to a dataframe, before we can write it out to Pinecone for indexing. \n",
    "\n",
    "There are two ways of doing it: \n",
    "1. Using pandas DataFrame, apply the single-node embedding model, and upsert to Pinecone in batches\n",
    "2. Using Spark Dataframe and use pandas UDFs to help us apply the embedding model on batches of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3167479a",
   "metadata": {},
   "source": [
    "### Method 1: Upsert to Pinecone in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f75e1f4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pdf = df.limit(1000).toPandas()\n",
    "display(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a3ffc5",
   "metadata": {},
   "source": [
    "Note: Pinecone free tier only allows one index. If you have existing indices, you need to delete them before you are able create a new index.\n",
    "\n",
    "We specify the similarity measure, embedding vector dimension within the index.\n",
    "\n",
    "Read documentation on how to [create index here](https://docs.pinecone.io/reference/create_index/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cd4904",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# We will use embeddings from this model to apply to our data\n",
    "model = SentenceTransformer(\n",
    "    \"all-MiniLM-L6-v2\", cache_folder=DA.paths.datasets\n",
    ")  # Use a pre-cached model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb2f368",
   "metadata": {},
   "source": [
    "Delete the index if it already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1621af",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pinecone_index_name = \"news\"\n",
    "\n",
    "if pinecone_index_name in pinecone.list_indexes():\n",
    "    pinecone.delete_index(pinecone_index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4d9a05",
   "metadata": {},
   "source": [
    "Create the index.\n",
    "\n",
    "We specify the index name (required), embedding vector dimension (required), and a custom similarity metric (cosine is the default) when creating our index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f1426a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# only create index if it doesn't exist\n",
    "if pinecone_index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        name=pinecone_index_name,\n",
    "        dimension=model.get_sentence_embedding_dimension(),\n",
    "        metric=\"cosine\",\n",
    "    )\n",
    "\n",
    "# now connect to the index\n",
    "pinecone_index = pinecone.Index(pinecone_index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3541f429",
   "metadata": {},
   "source": [
    "When the index has been created, we can now upsert vectors of data records to the index. `Upsert` means that we are writing the vectors into the index. \n",
    "\n",
    "Refer to this [documentation page](https://docs.pinecone.io/docs/python-client#indexupsert) to look at example code and vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edb7cb6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "for i in tqdm(range(0, len(pdf[\"title\"]), batch_size)):\n",
    "    # find end of batch\n",
    "    i_end = min(i + batch_size, len(pdf[\"title\"]))\n",
    "    # create IDs batch\n",
    "    ids = [str(x) for x in range(i, i_end)]\n",
    "    # create metadata batch\n",
    "    metadata = [{\"title\": title} for title in pdf[\"title\"][i:i_end]]\n",
    "    # create embeddings\n",
    "    embedding_title_batch = model.encode(pdf[\"title\"][i:i_end]).tolist()\n",
    "    # create records list for upsert\n",
    "    records = zip(ids, embedding_title_batch, metadata)\n",
    "    # upsert to Pinecone\n",
    "    pinecone_index.upsert(vectors=records)\n",
    "\n",
    "# check number of records in the index\n",
    "pinecone_index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427bf422",
   "metadata": {},
   "source": [
    "Once the vectors are upserted, we can now query the index directly! Notice that it returns us the similarity score in the result too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380e5c48",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = \"fish\"\n",
    "\n",
    "# create the query vector\n",
    "user_query = model.encode(query).tolist()\n",
    "\n",
    "# submit the query to the Pinecone index\n",
    "pinecone_answer = pinecone_index.query(user_query, top_k=3, include_metadata=True)\n",
    "\n",
    "for result in pinecone_answer[\"matches\"]:\n",
    "    print(f\"{round(result['score'], 2)}, {result['metadata']['title']}\")\n",
    "    print(\"-\" * 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a5ccae",
   "metadata": {},
   "source": [
    "### Method 2: Process with Spark and write to Pinecone with Spark\n",
    "\n",
    "Now that we have seen how to `upsert` with Pinecone, you may be curious whether we can use Spark DataFrame Writer (just like Weaviate) to write the entire dataframe out in a single operation. The answer is yes -- we will now take a look at how to do that and a spoiler alert is that you will need to use a Spark connector too! \n",
    "\n",
    "We first need to write a mapping function to map the tokenizer and embedding model onto batches of rows within the Spark DataFrame. We will be using a type of [pandas UDFs](https://www.databricks.com/blog/2020/05/20/new-pandas-udfs-and-python-type-hints-in-the-upcoming-release-of-apache-spark-3-0.html), called scalar iterator UDFs. \n",
    "\n",
    "> The function takes and outputs an iterator of pandas.Series.\n",
    "\n",
    "> The length of the whole output must be the same length of the whole input. Therefore, it can prefetch the data from the input iterator as long as the lengths of entire input and output are the same. The given function should take a single column as input.\n",
    "\n",
    "> It is also useful when the UDF execution requires expensive initialization of some state. \n",
    "\n",
    "We load the model once per partition of data, not per [batch](https://spark.apache.org/docs/latest/api/python/user_guide/sql/arrow_pandas.html#setting-arrow-batch-size), which is faster. \n",
    "\n",
    "For more documentation, refer [here](https://docs.databricks.com/udf/pandas.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33779cba",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import Iterator\n",
    "\n",
    "@pandas_udf(\"array<float>\")\n",
    "def create_embeddings_with_transformers(\n",
    "    sentences: Iterator[pd.Series],) -> Iterator[pd.Series]:\n",
    "    model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    for batch in sentences:\n",
    "        yield pd.Series(model.encode(batch).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a9acb4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "transformer_type = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedding_spark_df = (\n",
    "    df.limit(1000)\n",
    "    .withColumn(\"values\", create_embeddings_with_transformers(\"title\")) \n",
    "    .withColumn(\"namespace\", F.lit(transformer_type))\n",
    "    .withColumn(\"metadata\", F.to_json(F.struct(F.col(\"topic\").alias(\"TOPIC\"))))\n",
    "    # We select these columns because they are expected by the Spark-Pinecone connector\n",
    "    .select(\"id\", \"values\", \"namespace\", \"metadata\")\n",
    ")\n",
    "display(embedding_spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e3c27c",
   "metadata": {},
   "source": [
    "Repeat the same step as in Method 1 above to delete and recreate the index. Again, we need to delete the index because Pinecone free tier only allows one index.\n",
    "\n",
    "Note: This could take ~3 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8877f6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pinecone_index_name = \"news\"\n",
    "\n",
    "if pinecone_index_name in pinecone.list_indexes():\n",
    "    pinecone.delete_index(pinecone_index_name)\n",
    "\n",
    "# only create index if it doesn't exist\n",
    "model = SentenceTransformer(transformer_type)\n",
    "if pinecone_index_name not in pinecone.list_indexes():\n",
    "    pinecone.create_index(\n",
    "        name=pinecone_index_name,\n",
    "        dimension=model.get_sentence_embedding_dimension(),\n",
    "        metric=\"cosine\",\n",
    "    )\n",
    "\n",
    "# now connect to the index\n",
    "pinecone_index = pinecone.Index(pinecone_index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66e0059",
   "metadata": {},
   "source": [
    "Instead of writing in batches, you can now use Spark DataFrame Writer to write the data out to Pinecone directly.\n",
    "\n",
    "**IMPORTANT!!** You need to attach a Spark-Pinecone connector `s3://pinecone-jars/spark-pinecone-uberjar.jar` in the cluster you are using. Otherwise, this following command would fail. Refer to this [documentation](https://docs.pinecone.io/docs/databricks#setting-up-a-spark-cluster) if you need more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326fc1b4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(\n",
    "    embedding_spark_df.write.option(\"pinecone.apiKey\", pinecone_api_key)\n",
    "    .option(\"pinecone.environment\", pinecone_env)\n",
    "    .option(\"pinecone.projectName\", pinecone.whoami().projectname)\n",
    "    .option(\"pinecone.indexName\", pinecone_index_name)\n",
    "    .format(\"io.pinecone.spark.pinecone.Pinecone\")\n",
    "    .mode(\"append\")\n",
    "    .save()\n",
    ")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
